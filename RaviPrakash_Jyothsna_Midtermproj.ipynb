{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mlxtend in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.23.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (2.1.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (3.9.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlxtend) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jyothsna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "from IPython.display import display , HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Dataset (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV file\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    transactions = df.apply(lambda x: set(x.dropna().astype(str)), axis=1).tolist()  # Ensure all items are strings\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative paths for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Dictionary of available datasets\n",
    "datasets = {\n",
    "        1: 'Data/Amazon.csv',  # Amazon\n",
    "        2: 'Data/Best_Buy.csv',  # Costco\n",
    "        3: 'Data/IKEA.csv',  # Levis\n",
    "        4: 'Data/Puma.csv',  # Nike\n",
    "        5: 'Data/Target.csv'   # Walmart\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Items and Transactions for choosen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose a dataset:\n",
      "1. Amazon\n",
      "2. Best Buy\n",
      "3. IKEA\n",
      "4. Puma\n",
      "5. Target\n",
      "6. Exit\n",
      "\n",
      "Displaying Items and Transactions for: Amazon\n",
      "\n",
      "Data loaded successfully from Data/Amazon.csv\n",
      "\n",
      "--- List of Items in All Transactions ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coffee Maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fitness Tracker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headphones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kitchen Mixer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Laptop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Phone Charger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Portable Charger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Smart Speaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wireless Mouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yoga Mat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- List of Transactions ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item 1</th>\n",
       "      <th>Item 2</th>\n",
       "      <th>Item 3</th>\n",
       "      <th>Item 4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transaction ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Laptop</td>\n",
       "      <td>Headphones</td>\n",
       "      <td>Smart Speaker</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yoga Mat</td>\n",
       "      <td>Kitchen Mixer</td>\n",
       "      <td>Fitness Tracker</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smart Speaker</td>\n",
       "      <td>Phone Charger</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>Laptop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coffee Maker</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>Yoga Mat</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Headphones</td>\n",
       "      <td>Portable Charger</td>\n",
       "      <td>Fitness Tracker</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yoga Mat</td>\n",
       "      <td>Kitchen Mixer</td>\n",
       "      <td>Smart Speaker</td>\n",
       "      <td>Laptop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fitness Tracker</td>\n",
       "      <td>Headphones</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Coffee Maker</td>\n",
       "      <td>Smart Speaker</td>\n",
       "      <td>Phone Charger</td>\n",
       "      <td>Fitness Tracker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>Yoga Mat</td>\n",
       "      <td>Kitchen Mixer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Headphones</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>Coffee Maker</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fitness Tracker</td>\n",
       "      <td>Phone Charger</td>\n",
       "      <td>Kitchen Mixer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yoga Mat</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>Headphones</td>\n",
       "      <td>Coffee Maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Laptop</td>\n",
       "      <td>Smart Speaker</td>\n",
       "      <td>Portable Charger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>Fitness Tracker</td>\n",
       "      <td>Yoga Mat</td>\n",
       "      <td>Phone Charger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Smart Speaker</td>\n",
       "      <td>Headphones</td>\n",
       "      <td>Kitchen Mixer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Coffee Maker</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>Fitness Tracker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Yoga Mat</td>\n",
       "      <td>Smart Speaker</td>\n",
       "      <td>Headphones</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Phone Charger</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "      <td>Coffee Maker</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fitness Tracker</td>\n",
       "      <td>Kitchen Mixer</td>\n",
       "      <td>Laptop</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Headphones</td>\n",
       "      <td>Portable Charger</td>\n",
       "      <td>Smart Speaker</td>\n",
       "      <td>Wireless Mouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to load data using pandas\n",
    "def load_data_display(file_path):\n",
    "    try:\n",
    "        # Load the CSV file using pandas\n",
    "        data = pd.read_csv(file_path)\n",
    "        print(f\"Data loaded successfully from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to display items and transactions in table format with borders\n",
    "def display_items_and_transactions(data):\n",
    "    # Gather all unique items from the item columns\n",
    "    items = pd.concat([data['Item 1'], data['Item 2'], data['Item 3'], data['Item 4']]).dropna().unique()\n",
    "    \n",
    "    # Create a DataFrame for the list of items\n",
    "    items_df = pd.DataFrame(sorted(items), columns=[\"Items\"])\n",
    "    items_df.index = range(1, len(items_df) + 1)  # Set index to start from 1\n",
    "    print(\"\\n--- List of Items in All Transactions ---\")\n",
    "    display(HTML(items_df.to_html(border=1)))\n",
    "\n",
    "    # Create a DataFrame for transactions, with Transaction ID as index\n",
    "    transactions_df = data[['Transaction ID', 'Item 1', 'Item 2', 'Item 3', 'Item 4']].set_index('Transaction ID')\n",
    "    print(\"\\n--- List of Transactions ---\")\n",
    "    display(HTML(transactions_df.to_html(border=1)))\n",
    "\n",
    "# Dataset names for display\n",
    "dataset_names = ['Amazon', 'Best Buy', 'IKEA', 'Puma', 'Target']\n",
    "\n",
    "# Loop to prompt user for dataset selection\n",
    "while True:\n",
    "    print(\"Please choose a dataset:\")\n",
    "    for key, name in zip(datasets.keys(), dataset_names):\n",
    "        print(f\"{key}. {name}\")\n",
    "    print(\"6. Exit\")  # Option to exit\n",
    "\n",
    "    try:\n",
    "        choice = int(input(\"Enter the number corresponding to the dataset: \"))\n",
    "        if choice == 6:\n",
    "            print(\"Exiting the program.\")\n",
    "            break  # Exit the loop\n",
    "        elif choice in datasets:\n",
    "            print(f\"\\nDisplaying Items and Transactions for: {dataset_names[choice - 1]}\\n\")\n",
    "            file_path = datasets[choice]\n",
    "            transactions = load_data_display(file_path)\n",
    "            if transactions is not None:\n",
    "                display_items_and_transactions(transactions)  # Display items and transactions\n",
    "            break  # Valid dataset chosen and loaded, exit the loop\n",
    "        else:\n",
    "            print(\"Invalid choice. Please select a valid dataset.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number corresponding to the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1-itemsets for Brute Force\n",
    "def generate_1_itemsets(transactions):\n",
    "    itemset_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            if item in itemset_counts:\n",
    "                itemset_counts[item] += 1\n",
    "            else:\n",
    "                itemset_counts[item] = 1\n",
    "    return {frozenset([item]): count for item, count in itemset_counts.items()}\n",
    "\n",
    "# Generate k-itemsets from (k-1)-itemsets\n",
    "def generate_k_itemsets(prev_itemsets, k):\n",
    "    new_itemsets = []\n",
    "    prev_itemsets = list(prev_itemsets.keys())\n",
    "    \n",
    "    for i in range(len(prev_itemsets)):\n",
    "        for j in range(i + 1, len(prev_itemsets)):\n",
    "            candidate = prev_itemsets[i] | prev_itemsets[j]\n",
    "            if len(candidate) == k and candidate not in new_itemsets:\n",
    "                new_itemsets.append(candidate)\n",
    "    \n",
    "    return new_itemsets\n",
    "\n",
    "# Count support for itemsets\n",
    "def count_support(transactions, itemsets):\n",
    "    itemset_count = {itemset: 0 for itemset in itemsets}\n",
    "    \n",
    "    for transaction in transactions:\n",
    "        for itemset in itemsets:\n",
    "            if itemset.issubset(transaction):\n",
    "                itemset_count[itemset] += 1\n",
    "                \n",
    "    return itemset_count\n",
    "\n",
    "# Generate frequent itemsets for Brute Force\n",
    "def generate_frequent_itemsets(transactions, min_support):\n",
    "    frequent_itemsets = {}\n",
    "    total_transactions = len(transactions)\n",
    "    k = 1\n",
    "\n",
    "    # Generate 1-itemsets\n",
    "    current_itemsets = generate_1_itemsets(transactions)\n",
    "\n",
    "    while current_itemsets:\n",
    "        # Filter itemsets based on min_support\n",
    "        frequent_itemsets_k = {itemset: count for itemset, count in current_itemsets.items() if (count / total_transactions) * 100 >= min_support}\n",
    "        if not frequent_itemsets_k:\n",
    "            break\n",
    "        \n",
    "        # # Print the current k-itemsets and their counts\n",
    "        # print(f\"\\n{k}-itemsets:\")\n",
    "        # for itemset, count in frequent_itemsets_k.items():\n",
    "        #     support = (count / total_transactions) * 100\n",
    "        #     print(f\"Itemset: {set(itemset)}, Count: {count}, Support: {support:.2f}%\")\n",
    "        \n",
    "        frequent_itemsets.update(frequent_itemsets_k)\n",
    "        \n",
    "        # Generate next k-itemsets\n",
    "        k += 1\n",
    "        current_candidates = generate_k_itemsets(frequent_itemsets_k, k)\n",
    "        current_itemsets = count_support(transactions, current_candidates)\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "# Generate association rules from frequent itemsets\n",
    "def generate_association_rules(frequent_itemsets, min_confidence, total_transactions):\n",
    "    rules = []\n",
    "    \n",
    "    for itemset, support_count in frequent_itemsets.items():\n",
    "        for i in range(1, len(itemset)):\n",
    "            antecedent_list = [frozenset(antecedent) for antecedent in generate_combinations(list(itemset), i)]\n",
    "            for antecedent in antecedent_list:\n",
    "                consequent = itemset - antecedent\n",
    "                \n",
    "                if consequent:\n",
    "                    confidence = (support_count / frequent_itemsets[antecedent]) * 100 if antecedent in frequent_itemsets else 0\n",
    "                    support = (support_count / total_transactions) * 100\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, support, confidence))\n",
    "    \n",
    "    return rules\n",
    "\n",
    "# Helper function to generate combinations\n",
    "def generate_combinations(items, n):\n",
    "    if n == 0:\n",
    "        return [[]]\n",
    "    elif len(items) < n:\n",
    "        return []\n",
    "    else:\n",
    "        with_first = generate_combinations(items[1:], n - 1)\n",
    "        without_first = generate_combinations(items[1:], n)\n",
    "        \n",
    "        return [[items[0]] + comb for comb in with_first] + without_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode transactions to one-hot format\n",
    "def encode_transactions(transactions):\n",
    "    items = sorted(set(item for transaction in transactions for item in transaction))\n",
    "    encoded_df = pd.DataFrame(0, index=range(len(transactions)), columns=items)\n",
    "    \n",
    "    for idx, transaction in enumerate(transactions):\n",
    "        for item in transaction:\n",
    "            encoded_df.at[idx, item] = 1\n",
    "    \n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert rules into comparable sets (antecedents, consequents, support, confidence)\n",
    "def extract_rules(rule_list, is_bf=False):\n",
    "    extracted_rules = set()\n",
    "    if is_bf:\n",
    "        # Brute force rules already structured as (antecedent, consequent, support, confidence)\n",
    "        for antecedent, consequent, support, confidence in rule_list:\n",
    "            extracted_rules.add((frozenset(antecedent), frozenset(consequent), round(support, 2), round(confidence, 2)))\n",
    "    else:\n",
    "        # Apriori and FP-Growth: DataFrame format with 'antecedents', 'consequents', 'support', 'confidence'\n",
    "        for _, row in rule_list.iterrows():\n",
    "            extracted_rules.add((frozenset(row['antecedents']), frozenset(row['consequents']), round(row['support']*100, 2), round(row['confidence']*100, 2)))\n",
    "    return extracted_rules\n",
    "\n",
    "# Function to compare rules from different algorithms\n",
    "def compare_rules(association_rules_bf, rules_apriori, rules_fp):\n",
    "    # Extract rules from the three approaches\n",
    "    rules_bf_set = extract_rules(association_rules_bf, is_bf=True)\n",
    "    rules_apriori_set = extract_rules(rules_apriori)\n",
    "    rules_fp_set = extract_rules(rules_fp)\n",
    "\n",
    "    # Compare the exact content of the rules\n",
    "    same_rules = rules_bf_set == rules_apriori_set == rules_fp_set\n",
    "\n",
    "    if same_rules:\n",
    "        print(\"\\nThe algorithms produced identical association rules (including antecedents, consequents, support, and confidence).\")\n",
    "    else:\n",
    "        print(\"\\nThe algorithms produced different association rules.\")\n",
    "\n",
    "        # Print differences for debugging/comparison\n",
    "        print(\"\\nBrute Force Rules not in Apriori or FP-Growth:\")\n",
    "        print(rules_bf_set - rules_apriori_set - rules_fp_set)\n",
    "        \n",
    "        print(\"\\nApriori Rules not in Brute Force or FP-Growth:\")\n",
    "        print(rules_apriori_set - rules_bf_set - rules_fp_set)\n",
    "        \n",
    "        print(\"\\nFP-Growth Rules not in Brute Force or Apriori:\")\n",
    "        print(rules_fp_set - rules_bf_set - rules_apriori_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Function (Importing Datasets, Running Brute Force approach, Verifying the results with inbuilt packages of Apriori and FP-Growth and finding time complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose a dataset:\n",
      "1. Amazon\n",
      "2. Best Buy\n",
      "3. IKEA\n",
      "4. Puma\n",
      "5. Target\n",
      "6. Exit\n",
      "Loading data\n",
      "\n",
      "Brute Force Results:\n",
      "Rule: {'Headphones'} -> {'Wireless Mouse'}, Support: 25.00%, Confidence: 62.50%\n",
      "Rule: {'Wireless Mouse'} -> {'Headphones'}, Support: 25.00%, Confidence: 45.45%\n",
      "Rule: {'Smart Speaker'} -> {'Headphones'}, Support: 20.00%, Confidence: 50.00%\n",
      "Rule: {'Headphones'} -> {'Smart Speaker'}, Support: 20.00%, Confidence: 50.00%\n",
      "Rule: {'Laptop'} -> {'Wireless Mouse'}, Support: 20.00%, Confidence: 57.14%\n",
      "Rule: {'Wireless Mouse'} -> {'Laptop'}, Support: 20.00%, Confidence: 36.36%\n",
      "Rule: {'Yoga Mat'} -> {'Wireless Mouse'}, Support: 20.00%, Confidence: 57.14%\n",
      "Rule: {'Wireless Mouse'} -> {'Yoga Mat'}, Support: 20.00%, Confidence: 36.36%\n",
      "Rule: {'Wireless Mouse'} -> {'Coffee Maker'}, Support: 25.00%, Confidence: 45.45%\n",
      "Rule: {'Coffee Maker'} -> {'Wireless Mouse'}, Support: 25.00%, Confidence: 83.33%\n",
      "Rule: {'Smart Speaker'} -> {'Laptop'}, Support: 20.00%, Confidence: 50.00%\n",
      "Rule: {'Laptop'} -> {'Smart Speaker'}, Support: 20.00%, Confidence: 57.14%\n",
      "\n",
      "Apriori Results:\n",
      "         antecedents       consequents  support  confidence      lift\n",
      "0   (Wireless Mouse)    (Coffee Maker)     0.25    0.454545  1.515152\n",
      "1     (Coffee Maker)  (Wireless Mouse)     0.25    0.833333  1.515152\n",
      "2    (Smart Speaker)      (Headphones)     0.20    0.500000  1.250000\n",
      "3       (Headphones)   (Smart Speaker)     0.20    0.500000  1.250000\n",
      "4       (Headphones)  (Wireless Mouse)     0.25    0.625000  1.136364\n",
      "5   (Wireless Mouse)      (Headphones)     0.25    0.454545  1.136364\n",
      "6    (Smart Speaker)          (Laptop)     0.20    0.500000  1.428571\n",
      "7           (Laptop)   (Smart Speaker)     0.20    0.571429  1.428571\n",
      "8   (Wireless Mouse)          (Laptop)     0.20    0.363636  1.038961\n",
      "9           (Laptop)  (Wireless Mouse)     0.20    0.571429  1.038961\n",
      "10        (Yoga Mat)  (Wireless Mouse)     0.20    0.571429  1.038961\n",
      "11  (Wireless Mouse)        (Yoga Mat)     0.20    0.363636  1.038961\n",
      "\n",
      "FP-Growth Results:\n",
      "         antecedents       consequents  support  confidence      lift\n",
      "0       (Headphones)  (Wireless Mouse)     0.25    0.625000  1.136364\n",
      "1   (Wireless Mouse)      (Headphones)     0.25    0.454545  1.136364\n",
      "2    (Smart Speaker)      (Headphones)     0.20    0.500000  1.250000\n",
      "3       (Headphones)   (Smart Speaker)     0.20    0.500000  1.250000\n",
      "4    (Smart Speaker)          (Laptop)     0.20    0.500000  1.428571\n",
      "5           (Laptop)   (Smart Speaker)     0.20    0.571429  1.428571\n",
      "6   (Wireless Mouse)          (Laptop)     0.20    0.363636  1.038961\n",
      "7           (Laptop)  (Wireless Mouse)     0.20    0.571429  1.038961\n",
      "8         (Yoga Mat)  (Wireless Mouse)     0.20    0.571429  1.038961\n",
      "9   (Wireless Mouse)        (Yoga Mat)     0.20    0.363636  1.038961\n",
      "10  (Wireless Mouse)    (Coffee Maker)     0.25    0.454545  1.515152\n",
      "11    (Coffee Maker)  (Wireless Mouse)     0.25    0.833333  1.515152\n",
      "\n",
      "The algorithms produced identical association rules (including antecedents, consequents, support, and confidence).\n",
      "\n",
      "Timing Performance:\n",
      "Brute Force Time: 0.0000 seconds\n",
      "Apriori Time: 0.0155 seconds\n",
      "FP-Growth Time: 0.0030 seconds\n",
      "\n",
      "Fastest Algorithm: Brute Force with a time of 0.0000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jyothsna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jyothsna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Loop until valid input or exit\n",
    "    while True:\n",
    "        print(\"Please choose a dataset:\")\n",
    "        for key, name in zip(datasets.keys(), ['Amazon', 'Best Buy', 'IKEA', 'Puma', 'Target']):\n",
    "            print(f\"{key}. {name}\")\n",
    "        print(\"6. Exit\")  # Option to exit\n",
    "\n",
    "        try:\n",
    "            choice = int(input(\"Enter the number corresponding to the dataset: \"))\n",
    "            if choice == 6:\n",
    "                print(\"Exiting program.\")\n",
    "                return  # Exit the program\n",
    "            elif choice in datasets:\n",
    "                file_path = datasets[choice]\n",
    "                print(f\"Loading data\")\n",
    "                transactions = load_data(file_path)\n",
    "                break  # Valid dataset chosen, exit the loop\n",
    "            else:\n",
    "                print(\"Invalid choice. Please select a valid dataset.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number corresponding to the dataset.\")\n",
    "\n",
    "            # Input support and confidence thresholds\n",
    "    min_support = float(input(\"Enter minimum support threshold (0-100): \"))\n",
    "    min_confidence = float(input(\"Enter minimum confidence threshold (0-100): \"))    \n",
    "\n",
    "    # Brute Force Approach\n",
    "    start_time = time.time()\n",
    "    frequent_itemsets_bf = generate_frequent_itemsets(transactions, min_support)\n",
    "    total_transactions = len(transactions)\n",
    "    association_rules_bf = generate_association_rules(frequent_itemsets_bf, min_confidence, total_transactions)\n",
    "    brute_force_time = time.time() - start_time\n",
    "\n",
    "    # Apriori Approach\n",
    "    start_time = time.time()\n",
    "    encoded_df = encode_transactions(transactions)\n",
    "    frequent_itemsets_apriori = apriori(encoded_df, min_support=min_support / 100, use_colnames=True)\n",
    "    rules_apriori = association_rules(frequent_itemsets_apriori, metric=\"confidence\", min_threshold=min_confidence / 100)\n",
    "    apriori_time = time.time() - start_time\n",
    "\n",
    "    # FP-Growth Approach\n",
    "    start_time = time.time()\n",
    "    frequent_itemsets_fp = fpgrowth(encoded_df, min_support=min_support / 100, use_colnames=True)\n",
    "    rules_fp = association_rules(frequent_itemsets_fp, metric=\"confidence\", min_threshold=min_confidence / 100)\n",
    "    fp_growth_time = time.time() - start_time\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nBrute Force Results:\")\n",
    "    for antecedent, consequent, support, confidence in association_rules_bf:\n",
    "        print(f\"Rule: {set(antecedent)} -> {set(consequent)}, Support: {support:.2f}%, Confidence: {confidence:.2f}%\")\n",
    "\n",
    "    print(\"\\nApriori Results:\")\n",
    "    print(rules_apriori[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "    print(\"\\nFP-Growth Results:\")\n",
    "    print(rules_fp[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "    # Comparison of results (call the comparison function)\n",
    "    compare_rules(association_rules_bf, rules_apriori, rules_fp)    \n",
    "\n",
    "    # Timing performance\n",
    "    print(f\"\\nTiming Performance:\")\n",
    "    print(f\"Brute Force Time: {brute_force_time:.4f} seconds\")\n",
    "    print(f\"Apriori Time: {apriori_time:.4f} seconds\")\n",
    "    print(f\"FP-Growth Time: {fp_growth_time:.4f} seconds\")\n",
    "\n",
    "    # Determine which algorithm is fastest\n",
    "    fastest = min((\"Brute Force\", brute_force_time), (\"Apriori\", apriori_time), (\"FP-Growth\", fp_growth_time), key=lambda x: x[1])\n",
    "    print(f\"\\nFastest Algorithm: {fastest[0]} with a time of {fastest[1]:.4f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
